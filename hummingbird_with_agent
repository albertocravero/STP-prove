# Ho messo a posto i bug che c'erano (tra i quali avevo proprio sbagliato a definire l'agente non collegando i valori in ingresso all'ottimizzatore e quindi rimaneva costante).
# Avevo poi il problema che l'ottimizzatore tendeva ad andare in minimi locali e non riusciva a uscirne, quindi ho fatto una loss che penalizza molto se continua a non arrivare
# a x=L.



import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim


# Define the Radial Basis Function network
class RBFnet(nn.Module):
    def __init__(self, num_centers, final_point, lr,c1=1, c2=1 , sigma=1.0):
        super(RBFnet, self).__init__()

        self.num_centers = num_centers
        self.sigma = sigma
        self.centers = nn.Parameter(torch.linspace(0, final_point, num_centers)) # closed loop, 0≤x≤L, number of centers = final time (from paper) * mean frequency 
        self.weights = nn.Parameter(torch.normal(mean=np.pi/4, std=0.5, size=(num_centers,)), requires_grad=True) #initializing the weights from normal distribution 
        self.signs = torch.tensor([(-1)**i for i in range(num_centers)], dtype=torch.float32) #iniztializing the weight sign as opposite of the previous one

        self.loss_fn = nn.L1Loss() #L1 loss, squared cost was not converging
        #self.loss_fn = nn.MSELoss()
        self.optimizer = optim.Adam([self.weights], lr=lr) 

    #defining the gaussian
    def forward(self, x): 
        x = x.unsqueeze(1) if x.dim() == 1 else x
        G = torch.exp(-((x - self.centers)**2) / (2 * self.sigma**2)) * self.signs
        return torch.matmul(G, self.weights.clone())


# Define the agent
class RBF_agent():
    def __init__(self, env, num_centers, final_point, sigma,cont, c1 =1, c2=1, lr=0.01, weight_limit=np.pi):

        self.A_val = RBFnet(num_centers=num_centers, lr=lr, c1 = c1, c2= c2, final_point=final_point, sigma=sigma) #declaring the model

        self.final_point = final_point
        self.weight_limit = weight_limit 
        self.env = env #to get step evaluation
        self.c = [c1,c2] 
        self.last = 0
        self.added_loss = 1 # discussed later, initialized at 1 to be multiplied


    def choose_action(self, obs): #not very useful in this case
        return self.A_val.forward(obs[0])

    # Learn function (called during every timestep)
    def learn(self, state):
        
        ##
        # loss = c1 * dist_loss + c2 * integral_loss + (added_loss - 1)
        # dist_loss = abs(current position - L) -> every step it tries to go closer to the target
        # integral_loss = integral(A(x)**2) over the whole distance. Using integral instead of sum of actions because the discretization of the distance causes too few evaluations (i think)
        # added_loss -> to promote convergence (the env is done if vel<0, this causes minima on the countour (?) that are easier to converge in)
        # 
        ##
        target = torch.tensor(self.final_point, dtype=torch.float32) #final point converted into tensor
        pos, vel = self.env.evaluate_step(state, self.A_val(state[0])) #evaluating position and velocity starting from current state; the function will be evaluated during the opt step
        x_values = torch.linspace(0,self.final_point,100) #to evaluate integral loss
        self.A_val.optimizer.zero_grad()

        # defining added_loss. It promotes convergence if the episode truncates because of negative velocity. Shaped to be somehow dependant on the distance reached
        # every time the opt doesn't go in the direction of L, the loss increases
        if vel<0: 
            if pos <= 1.1*self.last: # promotes getting everytime closer to L

                if target-pos > 1: #if it's far enough, it can be multiplied. Replacing this operation with a sum does not convergence

                    self.added_loss *= target-pos

        # summed when the product doesn't make sense:
                else:

                    self.added_loss += target-pos # if the res is less than 1, product doesnt make sense. Summing is enough indeed

            elif pos<0: 
                    
                    self.added_loss += target-pos # trickiest spot to avoid. Multiplication can cause overlflow.

            print(self.added_loss,pos)

            self.last = pos # new last position is set.

        elif pos>target-0.05: #if the target is reached, the added loss is no more required.

            self.added_loss = 1
                 
        loss = self.c[0] * self.A_val.loss_fn(pos, target) + self.c[1] * torch.trapz(self.A_val(x_values)**2,x_values) + self.added_loss - 1 #self.A_val.loss_fn(vel,targets[1])

        loss.backward(retain_graph=True)

        self.A_val.optimizer.step()

        # clipping the values inside the limits
        with torch.no_grad():
            self.A_val.weights.clamp_(-self.weight_limit, self.weight_limit)
        return loss

# Define the HummingBird environment. Similar to custom gym envs
class HummingBird(gym.Env):
    def __init__(self, g=9.81, m=0.005, L=2, S=7.7e-2, dt=0.05, R=5.6e-2,c_A = 0.05):

        self.final_time_estimate = 0.9 + 0.2
        self.mean_freq = 50
        self.g = g
        self.m = m
        self.L = L
        self.S = S
        self.R = R
        self.dt = dt
        self.c_A = c_A
        self.states_ = [torch.tensor(0, dtype=torch.float32), torch.tensor(0, dtype=torch.float32)]

    def reset(self):

        self.states_ = [[torch.tensor(0, dtype=torch.float32), torch.tensor(0, dtype=torch.float32)]]
        return self.states_[0]

    def drag(self, x_dot):
        if x_dot == 0:
            x_dot = 1e-5
        #c_d = 7 / np.sqrt(np.abs(x_dot) * (self.S / self.R) / 1.460e-5) 
        c_d = 50 / np.sqrt(np.abs(x_dot) * (self.S / self.R) / 1.460e-5)

        return torch.tensor(0.5 * c_d * self.S * x_dot**2, dtype=torch.float32)

    def hummingbird_dynamics(self, state, action):

        F_g = self.m * self.g
        F_p = self.c_A * (action)**2
        F_d = self.drag(state[1].item())
        x_dot = state[1] + (F_p - F_g - F_d) * self.dt / self.m
        x = state[0] + x_dot * self.dt

        return x, x_dot

    def step(self, action): #to let the system evolve

        state = self.states_[-1]
        x, x_dot = self.hummingbird_dynamics(state, action) 
        cost = 0 #not used right now
        #state_ = [x, x_dot] 
        self.states_.append([x, x_dot])

        done = x >= self.L-0.05 or x_dot < 0 #truncation conditions: either the bird is 'close' to L or the velocity is negative

        return [x, x_dot], -cost, done, {}

    def evaluate_step(self, state, action): #to evaluate evolution of the system with different A(state) (used during optimization)
        x, x_dot = self.hummingbird_dynamics(state, action)

        return [x, x_dot]

# Test environment
#env = HummingBird(c_A=9.8*0.05*9/np.pi**2)
env = HummingBird(c_A=9.8*0.05*4/np.pi**2)

state = env.reset()

# Initialize the agent
num_centers = int(np.ceil(1 * env.final_time_estimate * env.mean_freq))

num_centers = 50 # to try
sigma = 0.02
#lr = 0.005
lr = 0.003
c = [1,5] #c[0] = distance_loss coefficient, c[1] = integral_loss coeff

#limit = 120/180*np.pi
limit = np.pi*6/18

agent = RBF_agent(num_centers=num_centers, env=env, final_point=env.L, cont = 0, weight_limit=limit, sigma=sigma, c1=c[0], c2 = c[1], lr=lr)


# Training loop + a lot of lists to track performances. To be cleaned.
num_epochs = 5000
x_values = torch.linspace(0, env.L, 1000, requires_grad=True)
final_distances = []
losses = []
#primo = agent.A_val(x_values)
total_cost = 0

best_model = {'loss': 1e5, 'epoch':0, 'net':[],'pos':[], 'vel':[]}
distances = []
int_losses = []
exp_losses = []
mean_weight = []
lenghts = []

for epoch in range(num_epochs):
    state = env.reset()
    done = False
    episode_loss = 0

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)
        reward = agent.learn(state)
       
        state = next_state

    episode_loss = reward
    final_distance = state[0]
    final_distances.append(final_distance.item())
    A_values = agent.A_val(x_values)
    positions = [i[0].item() for i in env.states_]
    lenghts.append(len(positions))
    loss_integral = torch.trapz(A_values**2, x=x_values)
    int_losses.append(loss_integral.item())
    exp_losses.append(episode_loss)

    if episode_loss < best_model['loss']:
        best_model['loss'] = episode_loss
        best_model['net'] = A_values
        best_model['epoch'] = epoch
        best_model['pos'] = positions
        best_model['vel'] = [i[1].item() for i in env.states_]

    losses.append(episode_loss)
    mean_weight.append(agent.A_val.weights.mean().item())
    if (epoch + 1) % 100 == 0:
        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {episode_loss}, Final Distance: {final_distance}')
        print(agent.A_val.weights)

best_epoch = best_model['epoch']


# Average loss of the last 100 epochs
last_100_losses = losses[-100:]
average_loss = sum(last_100_losses) / len(last_100_losses)
print(f'Average Loss of Last 100 Epochs: {average_loss}')
print('Best loss:', best_model['loss'], 'at epoch', best_model['epoch'], 'with final position:', best_model['pos'][-1], 'and final velocity:', best_model['vel'][-1])
print('lunghezze:',lenghts)
# Plots
plt.figure(1)
plt.plot(final_distances)
plt.title('Final distance over episodes')
plt.figure(2)
#plt.plot(x_values.detach().numpy(), primo.detach().numpy(), label='First')
plt.plot(x_values.detach().numpy(), 180/np.pi*A_values.detach().numpy(), label='Last')
plt.plot(x_values.detach().numpy(), 180/np.pi*best_model['net'].detach().numpy(), label=f'Best, epoch: {best_epoch}')
plt.legend(loc='best')
plt.xlabel('X')

plt.figure(3)
plt.plot([i.item() for i in exp_losses], label='Dist loss')
plt.plot(int_losses, label='Integral loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim(0,6)
plt.legend(loc='best')
plt.title('Loss over episodes')

plt.figure(4)
plt.plot(mean_weight)
plt.title('Mean weight over episodes')
plt.figure(5)
plt.plot(best_model['pos'],label = 'pos')
plt.plot(best_model['vel'], label = 'vel')
plt.legend(loc='best')

plt.plot()

plt.show()
